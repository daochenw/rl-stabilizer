{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to accompany the paper \"Constructions in combinatorics via neural networks and LP solvers\" by A Z Wagner\n",
    "# Code for conjecture 2.1, without the use of numba \n",
    "#\n",
    "# Please keep in mind that I am far from being an expert in reinforcement learning. \n",
    "# If you know what you are doing, you might be better off writing your own code.\n",
    "#\n",
    "# This code works on tensorflow version 1.14.0 and python version 3.6.3\n",
    "# It mysteriously breaks on other versions of python.\n",
    "# For later versions of tensorflow there seems to be a massive overhead in the predict function for some reason, and/or it produces mysterious errors.\n",
    "# Debugging these was way above my skill level.\n",
    "# If the code doesn't work, make sure you are using these versions of tf and python.\n",
    "# I used keras version 2.3.1, not sure if this is important, but I recommend this just to be safe.\n",
    "\n",
    "import networkx as nx #for various graph parameters, such as eigenvalues, macthing number, etc\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import load_model\n",
    "from statistics import mean\n",
    "from math import sqrt\n",
    "from numpy.random import choice\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stabilizer_search.search.brute_force import *\n",
    "from stabilizer_search.mat import X, T\n",
    "from stabilizer_search.mat import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 3\n",
    "chi = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plus = np.array([[1/sqrt(2)], [1/sqrt(2)]], dtype=np.complex_)\n",
    "# H = T*plus\n",
    "H = [[np.cos(np.pi/8)],[np.sin(np.pi/8)]]\n",
    "target_state = tensor(*([H]*n_qubits))\n",
    "real = np.allclose(np.imag(target_state), 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stabilizers = get_stabilizer_states(n_qubits, real_only=real)\n",
    "shuffle(stabilizers)\n",
    "n_stabilizers = len(stabilizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_true = [[0, 1, 0], [0, 0, 1]]\n",
    "# y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]\n",
    "# cce = keras.losses.SparseCategoricalCrossentropy()\n",
    "# cce(y_true, y_pred).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do_brute_force(target_state=target_state,n_qubits=3,chi=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Daochen: would be easier if could enforce the use of combinations\n",
    "# MYN = int(2**n)  #The length of the word we are generating. Here we are generating a Boolean function on n bits, so we create a 0-1 word of length 2^n\n",
    "\n",
    "MYN = int(chi)\n",
    "\n",
    "# LEARNING_RATE = 0.0001 #Increase this to make convergence faster, decrease if the algorithm gets stuck in local optima too often.\n",
    "LEARNING_RATE = 0.0001\n",
    "n_sessions = 500 #number of new sessions per iteration\n",
    "# default 93, 94 respectively\n",
    "percentile = 93 #top 100-X percentiled we are learning from\n",
    "super_percentile = 96 #top 100-X percentile that survives to next iteration\n",
    "\n",
    "# These are hyperparameters\n",
    "FIRST_LAYER_NEURONS = 128 #Number of neurons in the hidden layers.\n",
    "SECOND_LAYER_NEURONS = 64\n",
    "THIRD_LAYER_NEURONS = 4\n",
    "\n",
    "# n_actions = 2\n",
    "# Daochen: note that this parameter is not actually used anywhere.\n",
    "n_actions = n_stabilizers\n",
    "#The size of the alphabet. In this file we will assume this is 2. There are a few things we need to change when the alphabet size is larger,\n",
    "#such as one-hot encoding the input, and using categorical_crossentropy as a loss function.\n",
    "\n",
    "observation_space = 2*MYN \n",
    "\n",
    "# Leave this at 2*MYN. The input vector will have size 2*MYN, \n",
    "# where the first MYN letters encode our partial word (with zeros on\n",
    "# the positions we haven't considered yet), and the next MYN bits one-hot encode which letter we are considering now.\n",
    "# So e.g. [0,1,0,0,   0,0,1,0] means we have the partial word 01 and we are considering the third letter now.\n",
    "# Is there a better way to format the input to make it easier for the neural network to understand things?\n",
    "\n",
    "# Daochen: why should len_game have anything to do with MYN\n",
    "len_game = MYN \n",
    "state_dim = (observation_space,)\n",
    "\n",
    "INF = 1000000\n",
    "\n",
    "#Model structure: a sequential network with three hidden layers, sigmoid activation in the output.\n",
    "#I usually used relu activation in the hidden layers but play around to see what activation function and what optimizer works best.\n",
    "#It is important that the loss is binary cross-entropy if alphabet size is 2.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(FIRST_LAYER_NEURONS,  activation=\"relu\"))\n",
    "model.add(Dense(SECOND_LAYER_NEURONS, activation=\"relu\"))\n",
    "model.add(Dense(THIRD_LAYER_NEURONS, activation=\"relu\"))\n",
    "model.add(Dense(n_stabilizers, activation=\"softmax\"))\n",
    "model.build((None, observation_space))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate = LEARNING_RATE)) #Adam optimizer also works well, with lower learning rate\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcScore(state):\n",
    "    \"\"\"\n",
    "    Calculates the reward for a given word. \n",
    "    This function is very slow, it can be massively sped up with numba -- but numba doesn't support networkx yet, which is very convenient to use here\n",
    "    :param state: the first MYN letters of this param are the word that the neural network has constructed.\n",
    "\n",
    "    :returns: the reward (a real number). Higher is better, the network will try to maximize this.\n",
    "    \"\"\"\n",
    "\n",
    "    f = state[:MYN];\n",
    "    candidate_stabilizer_basis = [stabilizers[f[i]] for i in range(MYN)]\n",
    "    projector = ortho_projector(candidate_stabilizer_basis)\n",
    "    projection = np.linalg.norm(projector*target_state, 2)\n",
    "#     projection = 1\n",
    "    \n",
    "    score = projection\n",
    "    target = score\n",
    "    \n",
    "    if np.allclose(score, 1):\n",
    "        print('You found a stabilizer decomposition with (n_qubits,chi) = ', [n_qubits,chi])\n",
    "        print('The set of stabilizers is: ', f)\n",
    "    return target, score\n",
    "\n",
    "####No need to change anything below here. \n",
    "# Daochen: the agent argument will be the \"model\"\n",
    "def generate_session(agent, n_sessions, verbose = 1):\n",
    "    \"\"\"\n",
    "    Play n_session games using agent neural network.\n",
    "    Terminate when games finish \n",
    "\n",
    "    Code inspired by https://github.com/yandexdataschool/Practical_RL/blob/master/week01_intro/deep_crossentropy_method.ipynb\n",
    "    \"\"\"\n",
    "    states =  np.zeros([n_sessions, observation_space, len_game], dtype=int)\n",
    "    actions = np.zeros([n_sessions, len_game], dtype = int)\n",
    "    state_next = np.zeros([n_sessions,observation_space], dtype = int)\n",
    "    prob = np.zeros(n_sessions)\n",
    "    states[:,MYN,0] = 1\n",
    "    step = 0\n",
    "    total_target = np.zeros([n_sessions])\n",
    "#     total_target = np.zeros([n_sessions], dtype=complex)\n",
    "    total_score = np.zeros([n_sessions])\n",
    "    recordsess_time = 0\n",
    "    play_time = 0\n",
    "    scorecalc_time = 0\n",
    "    pred_time = 0\n",
    "    while (True):\n",
    "        step += 1\n",
    "        tic = time.time()\n",
    "        prob = agent.predict(states[:,:,step-1], batch_size = n_sessions) \n",
    "        pred_time += time.time()-tic\n",
    "\n",
    "        for i in range(n_sessions):\n",
    "            action = choice(n_stabilizers, p=prob[i])\n",
    "#             print('probability vector: ', prob[i],' and action chosen: ', action)\n",
    "            \n",
    "#             if np.random.rand() < prob[i]:\n",
    "#                 action = 1\n",
    "#             else:\n",
    "#                 action = 0\n",
    "            actions[i][step-1] = action\n",
    "            tic = time.time()\n",
    "            state_next[i] = states[i,:,step-1]\n",
    "            play_time += time.time()-tic\n",
    "            if (action > 0):\n",
    "                state_next[i][step-1] = action\n",
    "            state_next[i][MYN + step-1] = 0\n",
    "            if (step < MYN):\n",
    "                state_next[i][MYN + step] = 1\n",
    "#                 Daochen: terminal equals whether step equals MYN: I suppose meaning that an entire state has been generated\n",
    "            terminal = step == MYN\n",
    "            tic = time.time()\n",
    "            if terminal:\n",
    "#                 print('state_next[i]', state_next[i])\n",
    "                total_target[i], total_score[i] = calcScore(state_next[i])\n",
    "#                 print(\"total_score\", total_score[i])\n",
    "            scorecalc_time += time.time()-tic\n",
    "            tic = time.time()\n",
    "            if not terminal:\n",
    "                states[i,:,step] = state_next[i]\n",
    "            recordsess_time += time.time()-tic\n",
    "        if terminal:\n",
    "            break\n",
    "    #If you want, print out how much time each step has taken. This is useful to find the bottleneck in the program.\t\t\n",
    "    if (verbose):\n",
    "        print(\"Predict: \"+str(pred_time)+\", play: \" + str(play_time) +\", scorecalc: \" + str(scorecalc_time) +\", recordsess: \" + str(recordsess_time))\n",
    "    return states, actions, total_score, total_target\n",
    "\n",
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    This function was mostly taken from https://github.com/yandexdataschool/Practical_RL/blob/master/week01_intro/deep_crossentropy_method.ipynb\n",
    "    If this function is the bottleneck, it can easily be sped up using numba\n",
    "    \"\"\"\n",
    "    counter = n_sessions * (100.0 - percentile) / 100.0\n",
    "    reward_threshold = np.percentile(rewards_batch,percentile)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    elite_rewards = []\n",
    "    for i in range(len(states_batch)):\n",
    "        if rewards_batch[i] >= reward_threshold-0.0000001:\n",
    "            if (counter > 0) or (rewards_batch[i] >= reward_threshold+0.0000001):\n",
    "                for item in states_batch[i]:\n",
    "                    elite_states.append(item.tolist())\n",
    "                for item in actions_batch[i]:\n",
    "                    elite_actions.append(item)\n",
    "            counter -= 1\n",
    "    elite_states = np.array(elite_states, dtype = int)\n",
    "    elite_actions = np.array(elite_actions, dtype = int)\n",
    "    return elite_states, elite_actions\n",
    "\n",
    "def select_super_sessions(states_batch, actions_batch, rewards_batch, targets_batch, percentile=90):\n",
    "    \"\"\"\n",
    "    Select all the sessions that will survive to the next generation\n",
    "    Similar to select_elites function\n",
    "    If this function is the bottleneck, it can easily be sped up using numba\n",
    "    \"\"\"\n",
    "    counter = n_sessions * (100.0 - percentile) / 100.0\n",
    "    reward_threshold = np.percentile(rewards_batch,percentile)\n",
    "\n",
    "    super_states = []\n",
    "    super_actions = []\n",
    "    super_rewards = []\n",
    "    super_targets = []\n",
    "    for i in range(len(states_batch)):\n",
    "        if rewards_batch[i] >= reward_threshold-0.0000001:\n",
    "            if (counter > 0) or (rewards_batch[i] >= reward_threshold+0.0000001):\n",
    "                super_states.append(states_batch[i])\n",
    "                super_actions.append(actions_batch[i])\n",
    "                super_rewards.append(rewards_batch[i])\n",
    "                super_targets.append(targets_batch[i])\n",
    "                counter -= 1\n",
    "    super_states = np.array(super_states, dtype = int)\n",
    "    super_actions = np.array(super_actions, dtype = int)\n",
    "    super_rewards = np.array(super_rewards)\n",
    "    super_targets = np.array(super_targets)\n",
    "    return super_states, super_actions, super_rewards, super_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_states =  np.empty((0,len_game,observation_space), dtype = int)\n",
    "super_actions = np.array([], dtype = int)\n",
    "super_rewards = np.array([])\n",
    "super_targets= np.array([])\n",
    "sessgen_time = 0\n",
    "fit_time = 0\n",
    "score_time = 0\n",
    "\n",
    "myRand = random.randint(0,1000) #used in the filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(5000): #1000000 generations should be plenty\n",
    "    #generate new sessions\n",
    "    #performance can be improved with joblib\n",
    "    tic = time.time()\n",
    "#     sessions = states, actions, total_score, total_target\n",
    "    sessions = generate_session(model,n_sessions,0) #change 0 to 1 to print out how much time each step in generate_session takes \n",
    "    sessgen_time = time.time()-tic\n",
    "    tic = time.time()\n",
    "\n",
    "    states_batch = np.array(sessions[0], dtype = int)\n",
    "    actions_batch = np.array(sessions[1], dtype = int)\n",
    "    rewards_batch = np.array(sessions[2])\n",
    "    targets_batch = np.array(sessions[3])\n",
    "    \n",
    "    states_batch = np.transpose(states_batch,axes=[0,2,1])\n",
    "    states_batch = np.append(states_batch,super_states,axis=0)\n",
    "\n",
    "    if i>0:\n",
    "        actions_batch = np.append(actions_batch,np.array(super_actions),axis=0)\t\n",
    "    \n",
    "    rewards_batch = np.append(rewards_batch,super_rewards)\n",
    "    targets_batch = np.append(targets_batch,super_targets)\n",
    "\n",
    "    randomcomp_time = time.time()-tic \n",
    "    tic = time.time()\n",
    "\n",
    "    elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile=percentile) #pick the sessions to learn from\n",
    "    select1_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    super_sessions = select_super_sessions(states_batch, actions_batch, rewards_batch, targets_batch, percentile=super_percentile) #pick the sessions to survive\n",
    "    select2_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    super_sessions = [(super_sessions[0][i], super_sessions[1][i], super_sessions[2][i], super_sessions[3][i]) for i in range(len(super_sessions[2]))]\n",
    "    super_sessions.sort(key=lambda super_sessions: super_sessions[2],reverse=True)\n",
    "    select3_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    model.fit(elite_states, elite_actions) #learn from the elite sessions\n",
    "    fit_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "\n",
    "    super_states = [super_sessions[i][0] for i in range(len(super_sessions))]\n",
    "    super_actions = [super_sessions[i][1] for i in range(len(super_sessions))]\n",
    "    super_rewards = [super_sessions[i][2] for i in range(len(super_sessions))]\n",
    "    super_targets = [super_sessions[i][3] for i in range(len(super_sessions))]\n",
    "    \n",
    "#     print(super_states)\n",
    "\n",
    "    rewards_batch.sort()\n",
    "#     Daochen: why is it -100?\n",
    "    mean_all_reward = np.mean(rewards_batch[-100:])\n",
    "    mean_best_reward = np.mean(super_rewards)\n",
    "\n",
    "    score_time = time.time()-tic\n",
    "\n",
    "    print(\"\\n\" + str(i) +  \". Best individuals (reward): \" + str(np.flip(np.sort(super_rewards))))\n",
    "#     Daochen: note that sometimes it makes sense to add/remove np.flip below\n",
    "    print(\"\\n\" + str(i) +  \". Best individuals (target): \" + str(np.flip(np.sort(super_targets))))\n",
    "\n",
    "    #uncomment below line to print out how much time each step in this loop takes. \n",
    "     print(\"Mean reward: \" + str(mean_all_reward) + \"\\nSessgen: \" + str(sessgen_time) + \", other: \" + str(randomcomp_time) + \", select1: \" + str(select1_time) + \", select2: \" + str(select2_time) + \", select3: \" + str(select3_time) +  \", fit: \" + str(fit_time) + \", score: \" + str(score_time))\n",
    "#     if (i%25 == 1): #Write all important info to files every 20 iterations\n",
    "#         with open('best_species_pickle_'+str(myRand)+'.txt', 'wb') as fp:\n",
    "#             pickle.dump(super_actions, fp)\n",
    "#         with open('best_species_txt_'+str(myRand)+'.txt', 'w') as f:\n",
    "#             for item in super_actions:\n",
    "#                 f.write(str(item))\n",
    "#                 f.write(\"\\n\")\n",
    "#         with open('best_species_rewards_'+str(myRand)+'.txt', 'w') as f:\n",
    "#             for item in super_rewards:\n",
    "#                 f.write(str(item))\n",
    "#                 f.write(\"\\n\")\n",
    "#         with open('best_100_rewards_'+str(myRand)+'.txt', 'a') as f:\n",
    "#             f.write(str(mean_all_reward)+\"\\n\")\n",
    "#         with open('best_elite_rewards_'+str(myRand)+'.txt', 'a') as f:\n",
    "#             f.write(str(mean_best_reward)+\"\\n\")\n",
    "#     if (i%200==2): # To create a timeline, like in Figure 3\n",
    "#         with open('best_species_timeline_txt_'+str(myRand)+'.txt', 'a') as f:\n",
    "#             f.write(str(super_actions[0]))\n",
    "#             f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
