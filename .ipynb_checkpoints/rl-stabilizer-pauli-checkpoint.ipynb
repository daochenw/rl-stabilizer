{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to accompany the paper \"Constructions in combinatorics via neural networks and LP solvers\" by A Z Wagner\n",
    "# Code for conjecture 2.1, without the use of numba \n",
    "#\n",
    "# Please keep in mind that I am far from being an expert in reinforcement learning. \n",
    "# If you know what you are doing, you might be better off writing your own code.\n",
    "#\n",
    "# This code works on tensorflow version 1.14.0 and python version 3.6.3\n",
    "# It mysteriously breaks on other versions of python.\n",
    "# For later versions of tensorflow there seems to be a massive overhead in the predict function for some reason, and/or it produces mysterious errors.\n",
    "# Debugging these was way above my skill level.\n",
    "# If the code doesn't work, make sure you are using these versions of tf and python.\n",
    "# I used keras version 2.3.1, not sure if this is important, but I recommend this just to be safe.\n",
    "\n",
    "import networkx as nx #for various graph parameters, such as eigenvalues, macthing number, etc\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import load_model\n",
    "from statistics import mean\n",
    "from math import sqrt\n",
    "from numpy.random import choice\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import randstab as rs\n",
    "import utils as utils\n",
    "import kron_vec_product as kron_fast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stabilizer_search.search.brute_force import *\n",
    "from stabilizer_search.mat import X, T\n",
    "from stabilizer_search.mat import tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(2);\n",
    "X = np.array([[0,1],[1,0]]);\n",
    "Y = np.array([[0,-1j],[1j,0]]);\n",
    "Z = np.array([[1,0],[0,1]]);\n",
    "pauli = {0: I, 1: X, 2: Y, 3: Z};"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(state, pauli_number, n):\n",
    "    assert 0 <= pauli_number <= 4**n-1\n",
    "    implicit_pauli_string = np.zeros(n)\n",
    "    filler = utils.numberToBase(pauli_number,4)\n",
    "    implicit_pauli_string[-len(filler):] = filler\n",
    "    explicit_pauli_string = [pauli[implicit_pauli_string[i]] for i in range(n)]\n",
    "    state_new = kron_fast.kron_vec_prod(explicit_pauli_string, state)+state\n",
    "    if np.linalg.norm(state_new) == 0:\n",
    "#         print(\"NORM is zero\" - note norm can be zero from e.g. (1+X)|-> = 0)\n",
    "        return state  \n",
    "#     print(\"NORM is not zero\" - result of experiment: usually norm isn't zero, not a big problem)\n",
    "    assert np.linalg.norm(state) != 0\n",
    "    state_new = state_new/np.linalg.norm(state_new)\n",
    "    return state_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 2\n",
    "chi = 2\n",
    "n_paulis = 4**n_qubits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "H = [[np.cos(np.pi/8)],[np.sin(np.pi/8)]]\n",
    "Z0 = [[1],[0]]\n",
    "target_magic_state = tensor(*([H]*n_qubits))\n",
    "# tranpose needed to input base_stabilizer_states into update function\n",
    "base_stabilizer_states = [tensor(*([Z0]*n_qubits)).transpose()]*chi\n",
    "# base_stabilizer_states = [np.array([rs.random_stabilizer_state(n_qubits)]) for i in range(chi)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 16)                528       \n",
      "=================================================================\n",
      "Total params: 11,504\n",
      "Trainable params: 11,504\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Daochen: would be easier if could enforce the use of combinations\n",
    "# MYN = int(2**n)  #The length of the word we are generating. Here we are generating a Boolean function on n bits, so we create a 0-1 word of length 2^n\n",
    "\n",
    "MYN = int(chi)\n",
    "\n",
    "# LEARNING_RATE = 0.0001 #Increase this to make convergence faster, decrease if the algorithm gets stuck in local optima too often.\n",
    "LEARNING_RATE = 0.0001\n",
    "n_sessions = 500 #number of new sessions per iteration\n",
    "# default 93, 94 respectively\n",
    "percentile = 96 #top 100-X percentiled we are learning from\n",
    "super_percentile = 97 #top 100-X percentile that survives to next iteration\n",
    "\n",
    "# These are hyperparameters\n",
    "FIRST_LAYER_NEURONS = 128 #Number of neurons in the hidden layers.\n",
    "SECOND_LAYER_NEURONS = 64\n",
    "# THIRD_LAYER_NEURONS = 4\n",
    "THIRD_LAYER_NEURONS = 32\n",
    "\n",
    "# n_actions = 2\n",
    "# Daochen: note that this parameter is not actually used anywhere.\n",
    "n_actions = n_paulis\n",
    "#The size of the alphabet. In this file we will assume this is 2. There are a few things we need to change when the alphabet size is larger,\n",
    "#such as one-hot encoding the input, and using categorical_crossentropy as a loss function.\n",
    "\n",
    "observation_space = 2*MYN \n",
    "\n",
    "# Leave this at 2*MYN. The input vector will have size 2*MYN, \n",
    "# where the first MYN letters encode our partial word (with zeros on\n",
    "# the positions we haven't considered yet), and the next MYN bits one-hot encode which letter we are considering now.\n",
    "# So e.g. [0,1,0,0,   0,0,1,0] means we have the partial word 01 and we are considering the third letter now.\n",
    "# Is there a better way to format the input to make it easier for the neural network to understand things?\n",
    "\n",
    "# Daochen: why should len_game have anything to do with MYN\n",
    "len_game = MYN \n",
    "state_dim = (observation_space,)\n",
    "\n",
    "INF = 1000000\n",
    "\n",
    "#Model structure: a sequential network with three hidden layers, sigmoid activation in the output.\n",
    "#I usually used relu activation in the hidden layers but play around to see what activation function and what optimizer works best.\n",
    "#It is important that the loss is binary cross-entropy if alphabet size is 2.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(FIRST_LAYER_NEURONS,  activation=\"relu\"))\n",
    "model.add(Dense(SECOND_LAYER_NEURONS, activation=\"relu\"))\n",
    "model.add(Dense(THIRD_LAYER_NEURONS, activation=\"relu\"))\n",
    "model.add(Dense(n_paulis, activation=\"softmax\"))\n",
    "model.build((None, observation_space))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate = LEARNING_RATE)) #Adam optimizer also works well, with lower learning rate\n",
    "weights_init = model.get_weights()\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcScore(state):\n",
    "    \"\"\"\n",
    "    Calculates the reward for a given word. \n",
    "    This function is very slow, it can be massively sped up with numba -- but numba doesn't support networkx yet, which is very convenient to use here\n",
    "    :param state: the first MYN letters of this param are the word that the neural network has constructed.\n",
    "\n",
    "    :returns: the reward (a real number). Higher is better, the network will try to maximize this.\n",
    "    \"\"\"\n",
    "\n",
    "    f = state[:MYN];\n",
    "    candidate_stabilizer_basis = [update(base_stabilizer_states[i], f[i], n_qubits).transpose() for i in range(MYN)]\n",
    "    projector = ortho_projector(candidate_stabilizer_basis)\n",
    "    projection = np.linalg.norm(projector*target_magic_state, 2)\n",
    "#     projection = 1\n",
    "    \n",
    "    score = projection\n",
    "    target = score\n",
    "    \n",
    "    if np.allclose(score, 1):\n",
    "        print('You found a stabilizer decomposition with (n_qubits,chi) = ', [n_qubits,chi])\n",
    "        print('The set of stabilizers is: ', f)\n",
    "        return -1, -1\n",
    "    return target, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "####No need to change anything below here. \n",
    "# Daochen: the agent argument will be the \"model\"\n",
    "def generate_session(agent, n_sessions, verbose = 1):\n",
    "    \"\"\"\n",
    "    Play n_session games using agent neural network.\n",
    "    Terminate when games finish \n",
    "\n",
    "    Code inspired by https://github.com/yandexdataschool/Practical_RL/blob/master/week01_intro/deep_crossentropy_method.ipynb\n",
    "    \"\"\"\n",
    "    states =  np.zeros([n_sessions, observation_space, len_game], dtype=int)\n",
    "    actions = np.zeros([n_sessions, len_game], dtype = int)\n",
    "    state_next = np.zeros([n_sessions,observation_space], dtype = int)\n",
    "    prob = np.zeros(n_sessions)\n",
    "    states[:,MYN,0] = 1\n",
    "    step = 0\n",
    "    total_target = np.zeros([n_sessions])\n",
    "#     total_target = np.zeros([n_sessions], dtype=complex)\n",
    "    total_score = np.zeros([n_sessions])\n",
    "    recordsess_time = 0\n",
    "    play_time = 0\n",
    "    scorecalc_time = 0\n",
    "    pred_time = 0\n",
    "    while (True):\n",
    "        step += 1\n",
    "        tic = time.time()\n",
    "        prob = agent.predict(states[:,:,step-1], batch_size = n_sessions) \n",
    "        pred_time += time.time()-tic\n",
    "\n",
    "        for i in range(n_sessions):\n",
    "#             Daochen: important action selection step\n",
    "            action = choice(n_paulis, p=prob[i])\n",
    "#             print('probability vector: ', prob[i],' and action chosen: ', action)\n",
    "            actions[i][step-1] = action\n",
    "            tic = time.time()\n",
    "            state_next[i] = states[i,:,step-1]\n",
    "            play_time += time.time()-tic\n",
    "#             if (action > 0):\n",
    "#                 state_next[i][step-1] = action\n",
    "            state_next[i][step-1] = action\n",
    "            state_next[i][MYN + step-1] = 0\n",
    "            if (step < MYN):\n",
    "                state_next[i][MYN + step] = 1\n",
    "#                 Daochen: terminal equals whether step equals MYN: I suppose meaning that an entire state has been generated\n",
    "            terminal = step == MYN\n",
    "            tic = time.time()\n",
    "            if terminal:\n",
    "#                 print('state_next[i]', state_next[i])\n",
    "                total_target[i], total_score[i] = calcScore(state_next[i])\n",
    "                if total_target[i] == -1:\n",
    "                    return -1\n",
    "#                 print(\"total_score\", total_score[i])\n",
    "            scorecalc_time += time.time()-tic\n",
    "            tic = time.time()\n",
    "            if not terminal:\n",
    "                states[i,:,step] = state_next[i]\n",
    "            recordsess_time += time.time()-tic\n",
    "        if terminal:\n",
    "            break\n",
    "    #If you want, print out how much time each step has taken. This is useful to find the bottleneck in the program.\t\t\n",
    "    if (verbose):\n",
    "        print(\"Predict: \"+str(pred_time)+\", play: \" + str(play_time) +\", scorecalc: \" + str(scorecalc_time) +\", recordsess: \" + str(recordsess_time))\n",
    "    return states, actions, total_score, total_target\n",
    "\n",
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    This function was mostly taken from https://github.com/yandexdataschool/Practical_RL/blob/master/week01_intro/deep_crossentropy_method.ipynb\n",
    "    If this function is the bottleneck, it can easily be sped up using numba\n",
    "    \"\"\"\n",
    "    counter = n_sessions * (100.0 - percentile) / 100.0\n",
    "    reward_threshold = np.percentile(rewards_batch,percentile)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    elite_rewards = []\n",
    "    for i in range(len(states_batch)):\n",
    "        if rewards_batch[i] >= reward_threshold-0.0000001:\n",
    "            if (counter > 0) or (rewards_batch[i] >= reward_threshold+0.0000001):\n",
    "                for item in states_batch[i]:\n",
    "                    elite_states.append(item.tolist())\n",
    "                for item in actions_batch[i]:\n",
    "                    elite_actions.append(item)\n",
    "            counter -= 1\n",
    "    elite_states = np.array(elite_states, dtype = int)\n",
    "    elite_actions = np.array(elite_actions, dtype = int)\n",
    "    return elite_states, elite_actions\n",
    "\n",
    "def select_super_sessions(states_batch, actions_batch, rewards_batch, targets_batch, percentile=90):\n",
    "    \"\"\"\n",
    "    Select all the sessions that will survive to the next generation\n",
    "    Similar to select_elites function\n",
    "    If this function is the bottleneck, it can easily be sped up using numba\n",
    "    \"\"\"\n",
    "    counter = n_sessions * (100.0 - percentile) / 100.0\n",
    "    reward_threshold = np.percentile(rewards_batch,percentile)\n",
    "\n",
    "    super_states = []\n",
    "    super_actions = []\n",
    "    super_rewards = []\n",
    "    super_targets = []\n",
    "    for i in range(len(states_batch)):\n",
    "        if rewards_batch[i] >= reward_threshold-0.0000001:\n",
    "            if (counter > 0) or (rewards_batch[i] >= reward_threshold+0.0000001):\n",
    "                super_states.append(states_batch[i])\n",
    "                super_actions.append(actions_batch[i])\n",
    "                super_rewards.append(rewards_batch[i])\n",
    "                super_targets.append(targets_batch[i])\n",
    "                counter -= 1\n",
    "    super_states = np.array(super_states, dtype = int)\n",
    "    super_actions = np.array(super_actions, dtype = int)\n",
    "    super_rewards = np.array(super_rewards)\n",
    "    super_targets = np.array(super_targets)\n",
    "    return super_states, super_actions, super_rewards, super_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_states =  np.empty((0,len_game,observation_space), dtype = int)\n",
    "super_actions = np.array([], dtype = int)\n",
    "super_rewards = np.array([])\n",
    "super_targets= np.array([])\n",
    "sessgen_time = 0\n",
    "fit_time = 0\n",
    "score_time = 0\n",
    "\n",
    "myRand = random.randint(0,1000) #used in the filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 2ms/step - loss: 2.7026\n",
      "\n",
      "0. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.92387953 0.92387953 0.92387953 0.92387953\n",
      " 0.92387953 0.92387953 0.92387953 0.92387953 0.92387953 0.92387953\n",
      " 0.92387953 0.92387953 0.92387953 0.92387953 0.92387953]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7727\n",
      "\n",
      "1. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7635\n",
      "\n",
      "2. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7436\n",
      "\n",
      "3. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 2.6633\n",
      "\n",
      "4. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6355\n",
      "\n",
      "5. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 2.6777\n",
      "\n",
      "6. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6940\n",
      "\n",
      "7. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6645\n",
      "\n",
      "8. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6514\n",
      "\n",
      "9. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.7047\n",
      "\n",
      "10. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6997\n",
      "\n",
      "11. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6818\n",
      "\n",
      "12. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 2.6448\n",
      "\n",
      "13. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 2.6985\n",
      "\n",
      "14. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.5795\n",
      "\n",
      "15. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7106\n",
      "\n",
      "16. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7002\n",
      "\n",
      "17. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.6293\n",
      "\n",
      "18. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 2.6059\n",
      "\n",
      "19. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "Updated base_stabilizer_states at iteration:  20\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.7179\n",
      "\n",
      "20. Best individuals (reward): [0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856 0.98559856\n",
      " 0.98559856 0.98559856 0.98559856]\n",
      "You found a stabilizer decomposition with (n_qubits,chi) =  [2, 2]\n",
      "The set of stabilizers is:  [ 5 10]\n"
     ]
    }
   ],
   "source": [
    "for i in range(1000000): #1000000 generations should be plenty\n",
    "    #generate new sessions\n",
    "    #performance can be improved with joblib\n",
    "    tic = time.time()\n",
    "#     sessions = states, actions, total_score, total_target\n",
    "    sessions = generate_session(model,n_sessions,0) #change 0 to 1 to print out how much time each step in generate_session takes \n",
    "    if sessions == -1:\n",
    "        break\n",
    "    sessgen_time = time.time()-tic\n",
    "    tic = time.time()\n",
    "\n",
    "    states_batch = np.array(sessions[0], dtype = int)\n",
    "    actions_batch = np.array(sessions[1], dtype = int)\n",
    "    rewards_batch = np.array(sessions[2])\n",
    "    targets_batch = np.array(sessions[3])\n",
    "    \n",
    "    if i%20 == 0 and i > 1:\n",
    "        action = actions_batch[0]\n",
    "#         print('Action:', action)\n",
    "        print('Updated base_stabilizer_states at iteration: ', i)\n",
    "        base_stabilizer_states = [update(base_stabilizer_states[i], action[i], n_qubits) for i in range(MYN)]\n",
    "        model.set_weights(weights_init)\n",
    "#         print('debug', base_stabilizer_states)\n",
    "    \n",
    "    states_batch = np.transpose(states_batch,axes=[0,2,1])\n",
    "    states_batch = np.append(states_batch,super_states,axis=0)\n",
    "\n",
    "    if i>0:\n",
    "        actions_batch = np.append(actions_batch,np.array(super_actions),axis=0)\t\n",
    "    \n",
    "    rewards_batch = np.append(rewards_batch,super_rewards)\n",
    "    targets_batch = np.append(targets_batch,super_targets)\n",
    "\n",
    "    randomcomp_time = time.time()-tic \n",
    "    tic = time.time()\n",
    "\n",
    "    elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile=percentile) #pick the sessions to learn from\n",
    "    select1_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    super_sessions = select_super_sessions(states_batch, actions_batch, rewards_batch, targets_batch, percentile=super_percentile) #pick the sessions to survive\n",
    "    select2_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    super_sessions = [(super_sessions[0][i], super_sessions[1][i], super_sessions[2][i], super_sessions[3][i]) for i in range(len(super_sessions[2]))]\n",
    "    super_sessions.sort(key=lambda super_sessions: super_sessions[2],reverse=True)\n",
    "    select3_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    model.fit(elite_states, elite_actions) #learn from the elite sessions\n",
    "#     print('elite_states',elite_states)\n",
    "#     print('elite_actions',elite_actions)\n",
    "    fit_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "\n",
    "    super_states = [super_sessions[i][0] for i in range(len(super_sessions))]\n",
    "    super_actions = [super_sessions[i][1] for i in range(len(super_sessions))]\n",
    "    super_rewards = [super_sessions[i][2] for i in range(len(super_sessions))]\n",
    "    super_targets = [super_sessions[i][3] for i in range(len(super_sessions))]\n",
    "    \n",
    "#     print(super_states)\n",
    "\n",
    "    rewards_batch.sort()\n",
    "#     Daochen: why is it -100?\n",
    "    mean_all_reward = np.mean(rewards_batch[-100:])\n",
    "    mean_best_reward = np.mean(super_rewards)\n",
    "\n",
    "    score_time = time.time()-tic\n",
    "\n",
    "    print(\"\\n\" + str(i) +  \". Best individuals (reward): \" + str(np.flip(np.sort(super_rewards))))\n",
    "#     Daochen: note that sometimes it makes sense to add/remove np.flip below\n",
    "#     print(\"\\n\" +  \". Best individuals (target): \" + str(np.flip(np.sort(super_targets))))\n",
    "\n",
    "    #uncomment below line to print out how much time each step in this loop takes. \n",
    "#     print(\"Mean reward: \" + str(mean_all_reward) + \"\\nSessgen: \" + str(sessgen_time) + \", other: \" + str(randomcomp_time) + \", select1: \" + str(select1_time) + \", select2: \" + str(select2_time) + \", select3: \" + str(select3_time) +  \", fit: \" + str(fit_time) + \", score: \" + str(score_time))\n",
    "#     if (i%25 == 1): #Write all important info to files every 20 iterations\n",
    "#         with open('best_species_pickle_'+str(myRand)+'.txt', 'wb') as fp:\n",
    "#             pickle.dump(super_actions, fp)\n",
    "#         with open('best_species_txt_'+str(myRand)+'.txt', 'w') as f:\n",
    "#             for item in super_actions:\n",
    "#                 f.write(str(item))\n",
    "#                 f.write(\"\\n\")\n",
    "#         with open('best_species_rewards_'+str(myRand)+'.txt', 'w') as f:\n",
    "#             for item in super_rewards:\n",
    "#                 f.write(str(item))\n",
    "#                 f.write(\"\\n\")\n",
    "#         with open('best_100_rewards_'+str(myRand)+'.txt', 'a') as f:\n",
    "#             f.write(str(mean_all_reward)+\"\\n\")\n",
    "#         with open('best_elite_rewards_'+str(myRand)+'.txt', 'a') as f:\n",
    "#             f.write(str(mean_best_reward)+\"\\n\")\n",
    "#     if (i%200==2): # To create a timeline, like in Figure 3\n",
    "#         with open('best_species_timeline_txt_'+str(myRand)+'.txt', 'a') as f:\n",
    "#             f.write(str(super_actions[0]))\n",
    "#             f.write(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
