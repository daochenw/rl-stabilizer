{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to accompany the paper \"Constructions in combinatorics via neural networks and LP solvers\" by A Z Wagner\n",
    "# Code for conjecture 2.1, without the use of numba \n",
    "#\n",
    "# Please keep in mind that I am far from being an expert in reinforcement learning. \n",
    "# If you know what you are doing, you might be better off writing your own code.\n",
    "#\n",
    "# This code works on tensorflow version 1.14.0 and python version 3.6.3\n",
    "# It mysteriously breaks on other versions of python.\n",
    "# For later versions of tensorflow there seems to be a massive overhead in the predict function for some reason, and/or it produces mysterious errors.\n",
    "# Debugging these was way above my skill level.\n",
    "# If the code doesn't work, make sure you are using these versions of tf and python.\n",
    "# I used keras version 2.3.1, not sure if this is important, but I recommend this just to be safe.\n",
    "\n",
    "import networkx as nx #for various graph parameters, such as eigenvalues, macthing number, etc\n",
    "import random\n",
    "import numpy as np\n",
    "import copy\n",
    "import keras\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.models import load_model\n",
    "from statistics import mean\n",
    "from math import sqrt\n",
    "from numpy.random import choice\n",
    "import pickle\n",
    "import time\n",
    "import math\n",
    "import sympy\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stabilizer_search.search.brute_force import *\n",
    "from stabilizer_search.mat import X, Z, T\n",
    "from stabilizer_search.mat import tensor\n",
    "import randstab as rs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "T_state = np.array([[1],[np.exp(1j*np.pi/4)]])/np.sqrt(2)\n",
    "T_perp_state = Z*T_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_qubits = 4\n",
    "chi = 2\n",
    "H_state = [[np.cos(np.pi/8)],[np.sin(np.pi/8)]]\n",
    "# target_state = tensor(*([H_state]*n_qubits))\n",
    "target_state = (tensor(*([T_state]*n_qubits)) + tensor(*([T_perp_state]*n_qubits)))/np.sqrt(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of all stabilizer states = 36720\n"
     ]
    }
   ],
   "source": [
    "# Daochen: is there a way of knowing the number of real stabilizer states? O randomly generating real \n",
    "print('number of all stabilizer states =', sum(rs.number_of_states(n_qubits)))\n",
    "n_stabilizers_target = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target state is not real\n"
     ]
    }
   ],
   "source": [
    "is_target_state_real = all(np.isreal(target_state))\n",
    "if is_target_state_real:\n",
    "    print('Target state is real')\n",
    "else:\n",
    "    print('Target state is not real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of stabilizer states considered = 9560\n"
     ]
    }
   ],
   "source": [
    "stabilizers = [rs.random_stabilizer_state(n_qubits) for i in range(n_stabilizers_target)]\n",
    "L = {array.tobytes(): array for array in stabilizers}\n",
    "unique_stabilizers = list(L.values()) # [array([1, 3, 2, 4]), array([1, 2, 3, 4])]\n",
    "if is_target_state_real:\n",
    "    unique_real_stabilizers = list(filter(lambda x: all(np.isreal(x)), unique_stabilizers))\n",
    "    stabilizers = np.array(unique_real_stabilizers)\n",
    "stabilizers = np.array(unique_stabilizers)\n",
    "n_stabilizers = len(stabilizers)\n",
    "# why can this be sometimes bigger than number of all stabilizer states???\n",
    "print('number of stabilizer states considered =', n_stabilizers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check that the target is in the span\n",
    "# candidate_stabilizer_basis = [np.array([stabilizers[i,:]]).transpose() for i in range(n_stabilizers)]\n",
    "# projector = ortho_projector(candidate_stabilizer_basis)\n",
    "# projection = np.linalg.norm(projector*target_state, 2)\n",
    "# projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DON'T DO THIS! E.g. say you had |0>, |1>, |+> and you killed off |+> because it's linearly dependent, then you get a bad stabilizer decomposition of |+>!\n",
    "# _, inds = sympy.Matrix(candidate_stabilizer_states).T.rref()\n",
    "# candidate_stabilizer_states = candidate_stabilizer_states[inds,:]\n",
    "# n_candidates_actual = candidate_stabilizer_states.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 128)               640       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 9560)              315480    \n",
      "=================================================================\n",
      "Total params: 326,456\n",
      "Trainable params: 326,456\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Daochen: would be easier if could enforce the use of combinations\n",
    "# MYN = int(2**n)  #The length of the word we are generating. Here we are generating a Boolean function on n bits, so we create a 0-1 word of length 2^n\n",
    "\n",
    "MYN = int(chi)\n",
    "\n",
    "# LEARNING_RATE = 0.0001 #Increase this to make convergence faster, decrease if the algorithm gets stuck in local optima too often.\n",
    "LEARNING_RATE = 0.00001\n",
    "n_sessions = 500 #number of new sessions per iteration\n",
    "# default 93, 94 respectively\n",
    "percentile = 93 #top 100-X percentiled we are learning from\n",
    "super_percentile = 98 #top 100-X percentile that survives to next iteration\n",
    "\n",
    "# These are hyperparameters\n",
    "FIRST_LAYER_NEURONS = 128 #Number of neurons in the hidden layers.\n",
    "SECOND_LAYER_NEURONS = 64\n",
    "THIRD_LAYER_NEURONS = 32\n",
    "\n",
    "# n_actions = 2\n",
    "# Daochen: note that this parameter is not actually used anywhere.\n",
    "n_actions = n_stabilizers\n",
    "#The size of the alphabet. In this file we will assume this is 2. There are a few things we need to change when the alphabet size is larger,\n",
    "#such as one-hot encoding the input, and using categorical_crossentropy as a loss function.\n",
    "\n",
    "observation_space = 2*MYN \n",
    "\n",
    "# Leave this at 2*MYN. The input vector will have size 2*MYN, \n",
    "# where the first MYN letters encode our partial word (with zeros on\n",
    "# the positions we haven't considered yet), and the next MYN bits one-hot encode which letter we are considering now.\n",
    "# So e.g. [0,1,0,0,   0,0,1,0] means we have the partial word 01 and we are considering the third letter now.\n",
    "# Is there a better way to format the input to make it easier for the neural network to understand things?\n",
    "\n",
    "# Daochen: why should len_game have anything to do with MYN\n",
    "len_game = MYN \n",
    "state_dim = (observation_space,)\n",
    "\n",
    "INF = 1000000\n",
    "\n",
    "#Model structure: a sequential network with three hidden layers, sigmoid activation in the output.\n",
    "#I usually used relu activation in the hidden layers but play around to see what activation function and what optimizer works best.\n",
    "#It is important that the loss is binary cross-entropy if alphabet size is 2.\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(FIRST_LAYER_NEURONS,  activation=\"relu\"))\n",
    "model.add(Dense(SECOND_LAYER_NEURONS, activation=\"relu\"))\n",
    "model.add(Dense(THIRD_LAYER_NEURONS, activation=\"relu\"))\n",
    "model.add(Dense(n_stabilizers, activation=\"softmax\"))\n",
    "model.build((None, observation_space))\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=Adam(learning_rate = LEARNING_RATE)) #Adam optimizer also works well, with lower learning rate\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# candidate_stabilizer_basis = [np.array([candidate_stabilizer_states[i,:]]).transpose() for i in range(MYN)];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calcScore(state):\n",
    "    \"\"\"\n",
    "    Calculates the reward for a given word. \n",
    "    This function is very slow, it can be massively sped up with numba -- but numba doesn't support networkx yet, which is very convenient to use here\n",
    "    :param state: the first MYN letters of this param are the word that the neural network has constructed.\n",
    "\n",
    "    :returns: the reward (a real number). Higher is better, the network will try to maximize this.\n",
    "    \"\"\"\n",
    "\n",
    "    f = state[:MYN];\n",
    "    candidate_stabilizer_basis = [np.array([stabilizers[f[i],:]]).transpose() for i in range(MYN)]\n",
    "#     print(candidate_stabilizer_basis)\n",
    "    projector = ortho_projector(candidate_stabilizer_basis)\n",
    "    projection = np.linalg.norm(projector*target_state, 2)\n",
    "#     projection = 1\n",
    "    \n",
    "    score = projection\n",
    "    target = score\n",
    "    \n",
    "    if np.allclose(score, 1):\n",
    "        print('You found a stabilizer decomposition with (n_qubits,chi) = ', [n_qubits,chi])\n",
    "        print('The set of stabilizers is: ', f)\n",
    "        return -1, -1\n",
    "    return target, score\n",
    "\n",
    "####No need to change anything below here. \n",
    "# Daochen: the agent argument will be the \"model\"\n",
    "def generate_session(agent, n_sessions, verbose = 1):\n",
    "    \"\"\"\n",
    "    Play n_session games using agent neural network.\n",
    "    Terminate when games finish \n",
    "\n",
    "    Code inspired by https://github.com/yandexdataschool/Practical_RL/blob/master/week01_intro/deep_crossentropy_method.ipynb\n",
    "    \"\"\"\n",
    "    states =  np.zeros([n_sessions, observation_space, len_game], dtype=int)\n",
    "    actions = np.zeros([n_sessions, len_game], dtype = int)\n",
    "    state_next = np.zeros([n_sessions,observation_space], dtype = int)\n",
    "    prob = np.zeros(n_sessions)\n",
    "    states[:,MYN,0] = 1\n",
    "    step = 0\n",
    "    total_target = np.zeros([n_sessions])\n",
    "#     total_target = np.zeros([n_sessions], dtype=complex)\n",
    "    total_score = np.zeros([n_sessions])\n",
    "    recordsess_time = 0\n",
    "    play_time = 0\n",
    "    scorecalc_time = 0\n",
    "    pred_time = 0\n",
    "    while (True):\n",
    "        step += 1\n",
    "        tic = time.time()\n",
    "        prob = agent.predict(states[:,:,step-1], batch_size = n_sessions) \n",
    "        pred_time += time.time()-tic\n",
    "\n",
    "        for i in range(n_sessions):\n",
    "            action = choice(n_stabilizers, p=prob[i])\n",
    "            actions[i][step-1] = action\n",
    "            tic = time.time()\n",
    "            state_next[i] = states[i,:,step-1]\n",
    "            play_time += time.time()-tic\n",
    "            if (action > 0):\n",
    "                state_next[i][step-1] = action\n",
    "            state_next[i][MYN + step-1] = 0\n",
    "            if (step < MYN):\n",
    "                state_next[i][MYN + step] = 1\n",
    "#                 Daochen: terminal equals whether step equals MYN: I suppose meaning that an entire state has been generated\n",
    "            terminal = step == MYN\n",
    "            tic = time.time()\n",
    "            if terminal:\n",
    "#                 print('state_next[i]', state_next[i])\n",
    "                total_target[i], total_score[i] = calcScore(state_next[i])\n",
    "                if total_target[i] == -1:\n",
    "                    return -1\n",
    "#                 print(\"total_score\", total_score[i])\n",
    "            scorecalc_time += time.time()-tic\n",
    "            tic = time.time()\n",
    "            if not terminal:\n",
    "                states[i,:,step] = state_next[i]\n",
    "            recordsess_time += time.time()-tic\n",
    "        if terminal:\n",
    "            break\n",
    "    #If you want, print out how much time each step has taken. This is useful to find the bottleneck in the program.\t\t\n",
    "    if (verbose):\n",
    "        print(\"Predict: \"+str(pred_time)+\", play: \" + str(play_time) +\", scorecalc: \" + str(scorecalc_time) +\", recordsess: \" + str(recordsess_time))\n",
    "    return states, actions, total_score, total_target\n",
    "\n",
    "def select_elites(states_batch, actions_batch, rewards_batch, percentile=50):\n",
    "    \"\"\"\n",
    "    Select states and actions from games that have rewards >= percentile\n",
    "    :param states_batch: list of lists of states, states_batch[session_i][t]\n",
    "    :param actions_batch: list of lists of actions, actions_batch[session_i][t]\n",
    "    :param rewards_batch: list of rewards, rewards_batch[session_i]\n",
    "\n",
    "    :returns: elite_states,elite_actions, both 1D lists of states and respective actions from elite sessions\n",
    "\n",
    "    This function was mostly taken from https://github.com/yandexdataschool/Practical_RL/blob/master/week01_intro/deep_crossentropy_method.ipynb\n",
    "    If this function is the bottleneck, it can easily be sped up using numba\n",
    "    \"\"\"\n",
    "    counter = n_sessions * (100.0 - percentile) / 100.0\n",
    "    reward_threshold = np.percentile(rewards_batch,percentile)\n",
    "\n",
    "    elite_states = []\n",
    "    elite_actions = []\n",
    "    elite_rewards = []\n",
    "    for i in range(len(states_batch)):\n",
    "        if rewards_batch[i] >= reward_threshold-0.0000001:\n",
    "            if (counter > 0) or (rewards_batch[i] >= reward_threshold+0.0000001):\n",
    "                for item in states_batch[i]:\n",
    "                    elite_states.append(item.tolist())\n",
    "                for item in actions_batch[i]:\n",
    "                    elite_actions.append(item)\n",
    "            counter -= 1\n",
    "    elite_states = np.array(elite_states, dtype = int)\n",
    "    elite_actions = np.array(elite_actions, dtype = int)\n",
    "    return elite_states, elite_actions\n",
    "\n",
    "def select_super_sessions(states_batch, actions_batch, rewards_batch, targets_batch, percentile=90):\n",
    "    \"\"\"\n",
    "    Select all the sessions that will survive to the next generation\n",
    "    Similar to select_elites function\n",
    "    If this function is the bottleneck, it can easily be sped up using numba\n",
    "    \"\"\"\n",
    "    counter = n_sessions * (100.0 - percentile) / 100.0\n",
    "    reward_threshold = np.percentile(rewards_batch,percentile)\n",
    "\n",
    "    super_states = []\n",
    "    super_actions = []\n",
    "    super_rewards = []\n",
    "    super_targets = []\n",
    "    for i in range(len(states_batch)):\n",
    "        if rewards_batch[i] >= reward_threshold-0.0000001:\n",
    "            if (counter > 0) or (rewards_batch[i] >= reward_threshold+0.0000001):\n",
    "                super_states.append(states_batch[i])\n",
    "                super_actions.append(actions_batch[i])\n",
    "                super_rewards.append(rewards_batch[i])\n",
    "                super_targets.append(targets_batch[i])\n",
    "                counter -= 1\n",
    "    super_states = np.array(super_states, dtype = int)\n",
    "    super_actions = np.array(super_actions, dtype = int)\n",
    "    super_rewards = np.array(super_rewards)\n",
    "    super_targets = np.array(super_targets)\n",
    "    return super_states, super_actions, super_rewards, super_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "super_states =  np.empty((0,len_game,observation_space), dtype = int)\n",
    "super_actions = np.array([], dtype = int)\n",
    "super_rewards = np.array([])\n",
    "super_targets= np.array([])\n",
    "sessgen_time = 0\n",
    "fit_time = 0\n",
    "score_time = 0\n",
    "\n",
    "myRand = random.randint(0,1000) #used in the filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0. Best individuals (reward): [0.64086994 0.57735027 0.55901699 0.55901699 0.54006172 0.53452248\n",
      " 0.53452248 0.53033009 0.53033009 0.53033009]\n",
      "\n",
      "1. Best individuals (reward): [0.64086994 0.58630197 0.57735027 0.57735027 0.56694671 0.55901699\n",
      " 0.55901699 0.54772256 0.54006172 0.53452248 0.53452248]\n",
      "\n",
      "2. Best individuals (reward): [0.64086994 0.64086994 0.58630197 0.57735027 0.57735027 0.56694671\n",
      " 0.56694671 0.56694671 0.55901699 0.55901699 0.55901699]\n",
      "\n",
      "3. Best individuals (reward): [0.64086994 0.64086994 0.5976143  0.58630197 0.57735027 0.57735027\n",
      " 0.57735027 0.57735027 0.56694671 0.56694671]\n",
      "\n",
      "4. Best individuals (reward): [0.75       0.64086994 0.64086994 0.5976143  0.58630197 0.58630197\n",
      " 0.57735027 0.57735027 0.57735027 0.57735027 0.57735027]\n",
      "\n",
      "5. Best individuals (reward): [0.75       0.64086994 0.64086994 0.5976143  0.58630197 0.58630197\n",
      " 0.57735027 0.57735027 0.57735027 0.57735027]\n",
      "\n",
      "6. Best individuals (reward): [0.75       0.64086994 0.64086994 0.5976143  0.58630197 0.58630197\n",
      " 0.58630197 0.58630197 0.57735027 0.57735027 0.57735027 0.57735027\n",
      " 0.57735027 0.57735027]\n",
      "\n",
      "7. Best individuals (reward): [0.75       0.64086994 0.64086994 0.5976143  0.58630197 0.58630197\n",
      " 0.58630197 0.58630197 0.57735027 0.57735027]\n",
      "\n",
      "8. Best individuals (reward): [0.75       0.64086994 0.64086994 0.63737744 0.5976143  0.58630197\n",
      " 0.58630197 0.58630197 0.58630197 0.58630197 0.57735027 0.57735027]\n",
      "\n",
      "9. Best individuals (reward): [0.75       0.66143783 0.64086994 0.64086994 0.63737744 0.60553007\n",
      " 0.5976143  0.58630197 0.58630197 0.58630197]\n",
      "\n",
      "10. Best individuals (reward): [0.75       0.72886899 0.66143783 0.64086994 0.64086994 0.63737744\n",
      " 0.60553007 0.5976143  0.5976143  0.58630197]\n",
      "\n",
      "11. Best individuals (reward): [0.75       0.72886899 0.66143783 0.66143783 0.64549722 0.64086994\n",
      " 0.64086994 0.63737744 0.60553007 0.60553007]\n",
      "\n",
      "12. Best individuals (reward): [0.75       0.72886899 0.66143783 0.66143783 0.64549722 0.64086994\n",
      " 0.64086994 0.63737744 0.60553007 0.60553007 0.5976143 ]\n",
      "\n",
      "13. Best individuals (reward): [0.75       0.72886899 0.66143783 0.66143783 0.64549722 0.64086994\n",
      " 0.64086994 0.63737744 0.60553007 0.60553007 0.5976143 ]\n",
      "\n",
      "14. Best individuals (reward): [0.76376262 0.75       0.72886899 0.72886899 0.66143783 0.66143783\n",
      " 0.64549722 0.64086994 0.64086994 0.63737744]\n",
      "\n",
      "15. Best individuals (reward): [0.76376262 0.75       0.72886899 0.72886899 0.66143783 0.66143783\n",
      " 0.64549722 0.64086994 0.64086994 0.63737744 0.58630197]\n",
      "\n",
      "16. Best individuals (reward): [0.76376262 0.76376262 0.75       0.72886899 0.72886899 0.66143783\n",
      " 0.66143783 0.64549722 0.64086994 0.64086994 0.63737744]\n",
      "\n",
      "17. Best individuals (reward): [0.76376262 0.76376262 0.75592895 0.75       0.72886899 0.72886899\n",
      " 0.66143783 0.66143783 0.64549722 0.64086994 0.64086994]\n",
      "\n",
      "18. Best individuals (reward): [0.76376262 0.76376262 0.75592895 0.75       0.72886899 0.72886899\n",
      " 0.66143783 0.66143783 0.64549722 0.64086994 0.64086994]\n",
      "\n",
      "19. Best individuals (reward): [0.76376262 0.76376262 0.75592895 0.75       0.72886899 0.72886899\n",
      " 0.69436507 0.66143783 0.66143783 0.64549722]\n",
      "\n",
      "20. Best individuals (reward): [0.76376262 0.76376262 0.75592895 0.75       0.72886899 0.72886899\n",
      " 0.69436507 0.69436507 0.66143783 0.66143783 0.64549722]\n",
      "\n",
      "21. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.75592895 0.75       0.72886899\n",
      " 0.72886899 0.69436507 0.69436507 0.66143783 0.66143783]\n",
      "\n",
      "22. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.75592895 0.75\n",
      " 0.72886899 0.72886899 0.69436507 0.69436507]\n",
      "\n",
      "23. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.75592895 0.75\n",
      " 0.72886899 0.72886899 0.69436507 0.69436507 0.5976143 ]\n",
      "\n",
      "24. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.75592895 0.75\n",
      " 0.72886899 0.72886899 0.69436507 0.69436507 0.5976143 ]\n",
      "\n",
      "25. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.75592895\n",
      " 0.75       0.72886899 0.72886899 0.70710678]\n",
      "\n",
      "26. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.75592895\n",
      " 0.75       0.72886899 0.72886899 0.70710678 0.59160798]\n",
      "\n",
      "27. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.75592895 0.75       0.72886899 0.72886899 0.70710678]\n",
      "\n",
      "28. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.75592895 0.75       0.72886899 0.72886899 0.70710678]\n",
      "\n",
      "29. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.75592895 0.75       0.72886899 0.72886899 0.70710678]\n",
      "\n",
      "30. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.75592895 0.75       0.72886899 0.72886899 0.70710678]\n",
      "\n",
      "31. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.75592895 0.75       0.72886899 0.72886899 0.70710678]\n",
      "\n",
      "32. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.75592895 0.75       0.72886899 0.72886899]\n",
      "\n",
      "33. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.75592895 0.75       0.72886899 0.72886899]\n",
      "\n",
      "34. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.75592895 0.75       0.72886899 0.72886899]\n",
      "\n",
      "35. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.75592895 0.75       0.73192505]\n",
      "\n",
      "36. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.75592895 0.75       0.73192505 0.64549722]\n",
      "\n",
      "37. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.75592895 0.75       0.73192505]\n",
      "\n",
      "38. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.75592895 0.75       0.73192505]\n",
      "\n",
      "39. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.75592895 0.75       0.75      ]\n",
      "\n",
      "40. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.75592895 0.75       0.75      ]\n",
      "\n",
      "41. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.75592895 0.75      ]\n",
      "\n",
      "42. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.75592895 0.75      ]\n",
      "\n",
      "43. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.75592895 0.75      ]\n",
      "\n",
      "44. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.75592895 0.75      ]\n",
      "\n",
      "45. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.75592895 0.75      ]\n",
      "\n",
      "46. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.75592895 0.75      ]\n",
      "\n",
      "47. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.75592895 0.75      ]\n",
      "\n",
      "48. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.75592895]\n",
      "\n",
      "49. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.75592895]\n",
      "\n",
      "50. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.75592895]\n",
      "\n",
      "51. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.75592895]\n",
      "\n",
      "52. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.75592895]\n",
      "\n",
      "53. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "54. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "55. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.65828059]\n",
      "\n",
      "56. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.65828059]\n",
      "\n",
      "57. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.65828059]\n",
      "\n",
      "58. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "59. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "60. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "61. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "62. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.64549722]\n",
      "\n",
      "63. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "64. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.56273143]\n",
      "\n",
      "65. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.57735027 0.57735027]\n",
      "\n",
      "66. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.62678317]\n",
      "\n",
      "67. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.62678317]\n",
      "\n",
      "68. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.70710678]\n",
      "\n",
      "69. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.70710678]\n",
      "\n",
      "70. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.70710678]\n",
      "\n",
      "71. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.70710678]\n",
      "\n",
      "72. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.70710678]\n",
      "\n",
      "73. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.70710678]\n",
      "\n",
      "74. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.70710678]\n",
      "\n",
      "75. Best individuals (reward): [0.76376262 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "76. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "77. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.57735027 0.57735027\n",
      " 0.57735027]\n",
      "\n",
      "78. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.5976143 ]\n",
      "\n",
      "79. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.60553007]\n",
      "\n",
      "80. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.61237244]\n",
      "\n",
      "81. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "82. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "83. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "84. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "85. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "86. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "87. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "88. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "89. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "90. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.62678317]\n",
      "\n",
      "91. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.66143783]\n",
      "\n",
      "92. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.66143783]\n",
      "\n",
      "93. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.66143783]\n",
      "\n",
      "94. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.66143783]\n",
      "\n",
      "95. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.66143783]\n",
      "\n",
      "96. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "97. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262]\n",
      "\n",
      "98. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.66143783]\n",
      "\n",
      "99. Best individuals (reward): [0.77055175 0.76376262 0.76376262 0.76376262 0.76376262 0.76376262\n",
      " 0.76376262 0.76376262 0.76376262 0.76376262 0.66143783]\n"
     ]
    }
   ],
   "source": [
    "for i in range(100): #1000000 generations should be plenty\n",
    "    #generate new sessions\n",
    "    #performance can be improved with joblib\n",
    "    tic = time.time()\n",
    "#     sessions = states, actions, total_score, total_target\n",
    "    sessions = generate_session(model,n_sessions,0) #change 0 to 1 to print out how much time each step in generate_session takes \n",
    "    if sessions == -1:\n",
    "        break\n",
    "    sessgen_time = time.time()-tic\n",
    "    tic = time.time()\n",
    "\n",
    "    states_batch = np.array(sessions[0], dtype = int)\n",
    "    actions_batch = np.array(sessions[1], dtype = int)\n",
    "    rewards_batch = np.array(sessions[2])\n",
    "    targets_batch = np.array(sessions[3])\n",
    "    \n",
    "    states_batch = np.transpose(states_batch,axes=[0,2,1])\n",
    "    states_batch = np.append(states_batch,super_states,axis=0)\n",
    "\n",
    "    if i>0:\n",
    "        actions_batch = np.append(actions_batch,np.array(super_actions),axis=0)\t\n",
    "    \n",
    "    rewards_batch = np.append(rewards_batch,super_rewards)\n",
    "    targets_batch = np.append(targets_batch,super_targets)\n",
    "\n",
    "    randomcomp_time = time.time()-tic \n",
    "    tic = time.time()\n",
    "\n",
    "    elite_states, elite_actions = select_elites(states_batch, actions_batch, rewards_batch, percentile=percentile) #pick the sessions to learn from\n",
    "    select1_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    super_sessions = select_super_sessions(states_batch, actions_batch, rewards_batch, targets_batch, percentile=super_percentile) #pick the sessions to survive\n",
    "    select2_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    super_sessions = [(super_sessions[0][i], super_sessions[1][i], super_sessions[2][i], super_sessions[3][i]) for i in range(len(super_sessions[2]))]\n",
    "    super_sessions.sort(key=lambda super_sessions: super_sessions[2],reverse=True)\n",
    "    select3_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "    model.fit(elite_states, elite_actions, verbose=0) #learn from the elite sessions\n",
    "    fit_time = time.time()-tic\n",
    "\n",
    "    tic = time.time()\n",
    "\n",
    "    super_states = [super_sessions[i][0] for i in range(len(super_sessions))]\n",
    "    super_actions = [super_sessions[i][1] for i in range(len(super_sessions))]\n",
    "    super_rewards = [super_sessions[i][2] for i in range(len(super_sessions))]\n",
    "    super_targets = [super_sessions[i][3] for i in range(len(super_sessions))]\n",
    "    \n",
    "#     print(super_states)\n",
    "\n",
    "    rewards_batch.sort()\n",
    "#     Daochen: why is it -100?\n",
    "    mean_all_reward = np.mean(rewards_batch[-100:])\n",
    "    mean_best_reward = np.mean(super_rewards)\n",
    "\n",
    "    score_time = time.time()-tic\n",
    "\n",
    "#     if i%5 == 0 and i > 1:\n",
    "    print(\"\\n\" + str(i) +  \". Best individuals (reward): \" + str(np.flip(np.sort(super_rewards))))\n",
    "#     Daochen: note that sometimes it makes sense to add/remove np.flip below\n",
    "#     print(\"\\n\" + str(i) +  \". Best individuals (target): \" + str(np.flip(np.sort(super_targets))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
